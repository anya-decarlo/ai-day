Timestamp: 20250301-171947
Model Architecture: UNet
Optimizer: Adam
Loss Function: DiceCE
Learning Rate: 0.0001
Batch Size: 4
Epochs: 30
Augmentations: All
Dropout Rate: 0.0
Weight Decay: 1e-05
Patch Size: 64
Gradient Clipping: 1.0
Synthetic Samples: 200
Validation Ratio: 0.2
